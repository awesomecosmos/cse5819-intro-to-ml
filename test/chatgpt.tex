\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}   %defines equations
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }
 \usepackage{graphicx}
 \usepackage{titling}

\title{Part One: ChatGPT Self-Learning}
\author{Aayushi Verma}
\date{due 9/2/24}
 
 \usepackage{fancyhdr}
\fancypagestyle{plain}{%  the preset of fancyhdr 
    \fancyhf{} % clear all header and footer fields
    %\fancyfoot[R]{\includegraphics[width=2cm]{KULEUVEN_GENT_RGB_LOGO.png}}
    \fancyfoot[L]{\thedate}
    \fancyhead[L]{CSE 5819 Assignment \#1}
    \fancyhead[R]{\theauthor}
}
\makeatletter
\def\@maketitle{%
  \newpage
  \null
  \vskip 1em%
  \begin{center}%
  \let \footnote \thanks
    {\LARGE \@title \par}%
    \vskip 1em%
    %{\large \@date}%
  \end{center}%
  \par
  \vskip 1em}
\makeatother

\usepackage{lipsum}  
\usepackage{cmbright}

\begin{document}

\maketitle

%\noindent\begin{tabular}{@{}ll}
%    Student & \theauthor\\
%     Promotor &  dr. Gilles Callebaut\\
%     Co-promotors & ing. Jarne Van Mulders, ing. Guus Leenders
%\end{tabular}

\section{What are the different learning tasks in machine learning?}
In machine learning, there are several different learning tasks, each suited to different types of problems and data. These tasks can be broadly categorized into the following:

\begin{enumerate}
    \item \textbf{Supervised Learning}
    \begin{itemize}
        \item In supervised learning, the model is trained on labeled data, where each input is paired with a corresponding output. The goal is to learn a mapping from inputs to outputs that can generalize to new, unseen data.
        \item \textit{Examples}: Classification (e.g., spam detection), Regression (e.g., predicting house prices).
    \end{itemize}

    \item \textbf{Unsupervised Learning}
    \begin{itemize}
        \item In unsupervised learning, the model is trained on unlabeled data. The task is to discover patterns, structures, or relationships in the data without any predefined labels.
        \item \textit{Examples}: Clustering (e.g., customer segmentation), Dimensionality Reduction (e.g., PCA).
    \end{itemize}

    \item \textbf{Semi-Supervised Learning}
    \begin{itemize}
        \item Semi-supervised learning uses a combination of a small amount of labeled data and a large amount of unlabeled data to improve learning accuracy. It is especially useful when labeling data is expensive or time-consuming.
        \item \textit{Examples}: Combining labeled and unlabeled data to improve image classification.
    \end{itemize}

    \item \textbf{Reinforcement Learning}
    \begin{itemize}
        \item In reinforcement learning, an agent learns to make decisions by interacting with an environment. The agent receives rewards or penalties based on its actions and learns to maximize cumulative rewards over time.
        \item \textit{Examples}: Game playing (e.g., AlphaGo), Robotics.
    \end{itemize}

    \item \textbf{Self-Supervised Learning}
    \begin{itemize}
        \item Self-supervised learning is a type of unsupervised learning where the model generates labels from the input data itself. The model then learns to predict part of the data from other parts, effectively creating its own supervision.
        \item \textit{Examples}: Predicting the next word in a sentence, Image inpainting.
    \end{itemize}

    \item \textbf{Transfer Learning}
    \begin{itemize}
        \item Transfer learning involves transferring knowledge from one domain (the source domain) to another domain (the target domain). It is especially useful when the target domain has limited data.
        \item \textit{Examples}: Using a pre-trained image recognition model on a new, but related task.
    \end{itemize}

    \item \textbf{Multi-Task Learning}
    \begin{itemize}
        \item Multi-task learning involves training a model to perform multiple related tasks simultaneously. This allows the model to leverage shared information between tasks, leading to better generalization.
        \item \textit{Examples}: Simultaneous classification of multiple object categories in images.
    \end{itemize}

    \item \textbf{Few-Shot Learning}
    \begin{itemize}
        \item Few-shot learning aims to enable a model to learn from a very small number of labeled examples, often by leveraging prior knowledge or learning paradigms.
        \item \textit{Examples}: Image classification with only a few labeled examples per class.
    \end{itemize}

    \item \textbf{Zero-Shot Learning}
    \begin{itemize}
        \item Zero-shot learning aims to classify data into categories that the model has not been explicitly trained on by leveraging relationships between known and unknown classes.
        \item \textit{Examples}: Classifying images of unseen animal species.
    \end{itemize}

    \item \textbf{Active Learning}
    \begin{itemize}
        \item In active learning, the model iteratively selects the most informative data points from which it should learn, reducing the amount of labeled data required while improving learning efficiency.
        \item \textit{Examples}: Choosing which images to label next in a large dataset.
    \end{itemize}
\end{enumerate}

\section{What is the definition of supervised learning?}
\textbf{Supervised learning} is a type of machine learning where a model is trained on a labeled dataset. In this context, "labeled" means that each training example is paired with an output label or target that the model is expected to predict. The goal of supervised learning is to learn a mapping from inputs (features) to outputs (labels) so that the model can accurately predict the output for new, unseen data.

\textbf{Key Components:}
\begin{itemize}
    \item \textbf{Input Data (Features)}: The attributes or variables that describe the data points.
    \item \textbf{Output Data (Labels)}: The correct outcomes or categories associated with each input data point.
    \item \textbf{Training Process}: The model learns by finding patterns or correlations between the input features and the corresponding output labels.
    \item \textbf{Prediction}: Once trained, the model can be used to predict the label for new input data.
\end{itemize}

\textbf{Examples of Supervised Learning:}
\begin{itemize}
    \item \textbf{Classification}: Assigning an email as either "spam" or "not spam."
    \item \textbf{Regression}: Predicting the price of a house based on its features like size, location, and number of bedrooms.
\end{itemize}

\section{What are the widely used methods to solve supervised learning problems?}
There are several widely used methods to solve supervised learning problems, each suited to different types of data and tasks. Here’s a breakdown of some of the most common methods:

\begin{enumerate}
    \item \textbf{Linear Models}
    \begin{itemize}
        \item \textbf{Linear Regression}: Used for regression tasks; it models the relationship between the input features and the output as a linear combination of the features.
        \item \textbf{Logistic Regression}: Used for binary classification tasks; it models the probability that a given input belongs to a particular class.
    \end{itemize}
    
    \item \textbf{Decision Trees and Ensemble Methods}
    \begin{itemize}
        \item \textbf{Decision Trees}: A tree-like model of decisions, where each internal node represents a feature, each branch represents a decision rule, and each leaf node represents an outcome (class or value).
        \item \textbf{Random Forest}: An ensemble method that builds multiple decision trees and merges them to get a more accurate and stable prediction.
        \item \textbf{Gradient Boosting Machines (GBM)}: An ensemble technique that builds trees sequentially, with each new tree correcting errors made by the previous ones.
        \begin{itemize}
            \item \textbf{XGBoost}: A highly efficient and popular implementation of gradient boosting.
            \item \textbf{LightGBM}: A faster and more memory-efficient implementation of gradient boosting.
            \item \textbf{CatBoost}: Another gradient boosting algorithm that handles categorical data efficiently.
        \end{itemize}
    \end{itemize}

    \item \textbf{Support Vector Machines (SVM)}
    \begin{itemize}
        \item \textbf{Support Vector Machines}: A powerful method for both classification and regression tasks. SVMs find the optimal hyperplane that separates the data into different classes with the maximum margin.
    \end{itemize}

    \item \textbf{k-Nearest Neighbors (k-NN)}
    \begin{itemize}
        \item \textbf{k-Nearest Neighbors}: A simple, instance-based learning method where the output for a new data point is determined by the majority class (in classification) or average (in regression) of its k-nearest neighbors in the feature space.
    \end{itemize}

    \item \textbf{Neural Networks}
    \begin{itemize}
        \item \textbf{Artificial Neural Networks (ANNs)}: A model inspired by the human brain's structure, capable of capturing complex patterns in data. They consist of layers of interconnected neurons where each layer transforms the input data progressively.
        \item \textbf{Convolutional Neural Networks (CNNs)}: Specialized neural networks designed for processing grid-like data such as images. They are particularly effective in image recognition tasks.
        \item \textbf{Recurrent Neural Networks (RNNs)}: Neural networks designed for sequential data, such as time series or natural language. Variants like Long Short-Term Memory (LSTM) networks are widely used.
    \end{itemize}

    \item \textbf{Naive Bayes}
    \begin{itemize}
        \item \textbf{Naive Bayes Classifier}: A probabilistic classifier based on Bayes' theorem, assuming independence between features. Despite its simplicity, it’s effective for text classification tasks like spam detection.
    \end{itemize}

    \item \textbf{Bayesian Methods}
    \begin{itemize}
        \item \textbf{Bayesian Linear Regression}: A version of linear regression where the model parameters are treated as random variables with prior distributions.
        \item \textbf{Bayesian Networks}: Graphical models that represent probabilistic relationships among variables.
    \end{itemize}

    \item \textbf{Discriminant Analysis}
    \begin{itemize}
        \item \textbf{Linear Discriminant Analysis (LDA)}: A method used for classification that finds a linear combination of features that best separates two or more classes.
        \item \textbf{Quadratic Discriminant Analysis (QDA)}: Similar to LDA, but allows for quadratic decision boundaries.
    \end{itemize}

    \item \textbf{Gaussian Processes}
    \begin{itemize}
        \item \textbf{Gaussian Process Regression (GPR)}: A non-parametric approach that models the distribution over functions and makes predictions by considering the joint distribution of observed data and unseen data.
    \end{itemize}

    \item \textbf{Ensemble Learning}
    \begin{itemize}
        \item \textbf{Bagging (Bootstrap Aggregating)}: An ensemble method that improves model stability and accuracy by training multiple models on different subsets of the data and averaging their predictions.
        \item \textbf{Stacking}: An ensemble method where multiple models are trained, and their outputs are used as inputs for another model (meta-model), which makes the final prediction.
    \end{itemize}

    \item \textbf{Regularization Techniques}
    \begin{itemize}
        \item \textbf{Ridge Regression (L2 Regularization)}: A linear regression model that penalizes the square of the coefficients, helping to prevent overfitting.
        \item \textbf{Lasso Regression (L1 Regularization)}: Similar to ridge regression, but it penalizes the absolute value of the coefficients, which can lead to feature selection by shrinking some coefficients to zero.
    \end{itemize}

    \item \textbf{Deep Learning}
    \begin{itemize}
        \item \textbf{Deep Neural Networks (DNNs)}: Neural networks with many layers, capable of learning high-level abstractions from data. They are widely used in tasks like image recognition, natural language processing, and speech recognition.
    \end{itemize}
\end{enumerate}

\section{What is the definition of unsupervised learning?}
\textbf{Unsupervised learning} is a type of machine learning where the model is trained on a dataset without labeled outputs. In this approach, the model tries to learn the underlying structure, patterns, or distribution in the data without any explicit guidance on what the correct output should be. The goal is to discover hidden patterns, groupings, or features within the data.

\textbf{Key Characteristics:}
\begin{itemize}
    \item \textbf{No Labeled Data}: The data provided to the model does not include labels or target outcomes.
    \item \textbf{Pattern Discovery}: The model identifies patterns, relationships, or structures within the data based on the similarities and differences between data points.
    \item \textbf{Exploratory}: Unsupervised learning is often exploratory in nature, aiming to understand the underlying structure of the data.
\end{itemize}

\textbf{Examples of Unsupervised Learning:}
\begin{itemize}
    \item \textbf{Clustering}: Grouping similar data points together based on certain criteria.
    \begin{itemize}
        \item \textit{Example}: Segmenting customers into distinct groups based on purchasing behavior.
    \end{itemize}
    \item \textbf{Dimensionality Reduction}: Reducing the number of features while retaining the essential information.
    \begin{itemize}
        \item \textit{Example}: Principal Component Analysis (PCA) for reducing the complexity of data.
    \end{itemize}
    \item \textbf{Anomaly Detection}: Identifying outliers or unusual data points that do not fit the general pattern.
    \begin{itemize}
        \item \textit{Example}: Detecting fraudulent transactions in financial data.
    \end{itemize}
    \item \textbf{Association Rule Learning}: Finding rules that describe large portions of the data.
    \begin{itemize}
        \item \textit{Example}: Market basket analysis, where the model identifies items that frequently co-occur in transactions.
    \end{itemize}
\end{itemize}

In summary, unsupervised learning involves analyzing and finding hidden structures in unlabeled data, making it useful for exploratory data analysis, pattern recognition, and feature extraction.

\section{What are the widely used methods to solve unsupervised learning problems?}
Unsupervised learning involves a variety of methods designed to discover patterns, structures, and relationships within unlabeled data. Here are some of the most widely used methods to solve unsupervised learning problems:

\begin{enumerate}
    \item \textbf{Clustering}
    \begin{itemize}
        \item \textbf{k-Means Clustering}: Partitions the data into k distinct clusters based on feature similarity. It assigns each data point to the nearest cluster center and iteratively refines the clusters.
        \item \textbf{Hierarchical Clustering}: Builds a tree-like structure of clusters, where each level represents a different granularity of grouping. It can be agglomerative (bottom-up) or divisive (top-down).
        \item \textbf{DBSCAN (Density-Based Spatial Clustering of Applications with Noise)}: Identifies clusters based on the density of data points, allowing the discovery of clusters of arbitrary shapes and handling noise (outliers) effectively.
        \item \textbf{Gaussian Mixture Models (GMMs)}: Assumes that the data is generated from a mixture of several Gaussian distributions, each representing a cluster. It uses the Expectation-Maximization (EM) algorithm to estimate the parameters.
    \end{itemize}

    \item \textbf{Dimensionality Reduction}
    \begin{itemize}
        \item \textbf{Principal Component Analysis (PCA)}: Reduces the dimensionality of the data by transforming it into a set of linearly uncorrelated variables (principal components), capturing the maximum variance.
        \item \textbf{t-Distributed Stochastic Neighbor Embedding (t-SNE)}: Reduces dimensions by modeling the probability distribution of pairwise distances between data points, particularly effective for visualizing high-dimensional data in 2D or 3D.
        \item \textbf{Autoencoders}: A type of neural network used to learn a compressed representation of the data. It consists of an encoder that compresses the data and a decoder that reconstructs it.
        \item \textbf{Independent Component Analysis (ICA)}: Separates a multivariate signal into additive, independent components, often used in signal processing and source separation.
    \end{itemize}

    \item \textbf{Association Rule Learning}
    \begin{itemize}
        \item \textbf{Apriori Algorithm}: Identifies frequent itemsets in transactional data and generates association rules, such as "If a customer buys bread, they are likely to buy butter."
        \item \textbf{Eclat Algorithm}: An improvement over Apriori, it focuses on itemset intersections to find frequent itemsets more efficiently.
    \end{itemize}

    \item \textbf{Anomaly Detection}
    \begin{itemize}
        \item \textbf{Isolation Forest}: Detects anomalies by randomly partitioning data points and isolating those that are fewer in number and different from the rest.
        \item \textbf{One-Class SVM}: A variant of Support Vector Machines that identifies the majority class (normal data) and classifies outliers as anomalies.
        \item \textbf{LOF (Local Outlier Factor)}: Measures the local density deviation of a data point with respect to its neighbors, identifying those with significantly lower density as outliers.
    \end{itemize}

    \item \textbf{Latent Variable Models}
    \begin{itemize}
        \item \textbf{Latent Dirichlet Allocation (LDA)}: A generative probabilistic model used for topic modeling, where documents are represented as mixtures of topics, and topics are mixtures of words.
        \item \textbf{Factor Analysis}: Models the data as linear combinations of underlying factors, used for identifying the hidden variables that explain observed data.
    \end{itemize}

    \item \textbf{Self-Organizing Maps (SOMs)}
    \begin{itemize}
        \item \textbf{Self-Organizing Maps}: A type of artificial neural network trained using unsupervised learning to produce a low-dimensional (typically 2D) representation of the input space, preserving the topological properties.
    \end{itemize}

    \item \textbf{Dimensionality Reduction with Manifold Learning}
    \begin{itemize}
        \item \textbf{Locally Linear Embedding (LLE)}: Captures the local neighborhood structure of the data and projects it into a lower-dimensional space.
        \item \textbf{Multidimensional Scaling (MDS)}: Aims to place each data point in a lower-dimensional space while preserving the pairwise distances as much as possible.
    \end{itemize}

    \item \textbf{Clustering-Based Methods for Image and Text Data}
    \begin{itemize}
        \item \textbf{Bag of Words (BoW)}: Represents text data as a bag of words, ignoring grammar and word order, and uses clustering techniques to identify patterns or topics.
        \item \textbf{Word2Vec}: Embeds words into vectors based on their context in the corpus, allowing clustering and pattern recognition in text data.
    \end{itemize}

    \item \textbf{Generative Models}
    \begin{itemize}
        \item \textbf{Generative Adversarial Networks (GANs)}: Consists of a generator and a discriminator; the generator creates synthetic data, and the discriminator evaluates how close the synthetic data is to real data, leading to the generation of realistic data.
        \item \textbf{Variational Autoencoders (VAEs)}: A type of autoencoder that learns the distribution of the data in a continuous latent space, allowing for data generation.
    \end{itemize}

    \item \textbf{Matrix Factorization}
    \begin{itemize}
        \item \textbf{Singular Value Decomposition (SVD)}: Factorizes a matrix into three other matrices and is widely used in collaborative filtering for recommendation systems.
        \item \textbf{Non-negative Matrix Factorization (NMF)}: Similar to SVD but with the constraint that the resulting matrices have no negative elements, used in topics modeling and image processing.
    \end{itemize}
\end{enumerate}


\end{document}
