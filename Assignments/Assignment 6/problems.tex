\section{Problems}
%% TEMPLATE STUFF - WORK BELOW THIS BLOCK
%\includegraphics[width=1\textwidth]{media/hw5_q1.png}
%\begin{align}
%    P(I=m | iPad = y) &= \frac{1}{4} \label{eq:q1_1} \nonumber \\
%    P(I=m | iPad = n) &= \frac{4}{4} = 1
%\end{align}

\subsection{Question One}
The method called “bagging” was not discussed in class lecture. “Bagging” is a short name of Bootstrap Aggregating. We would like you to self-study this concept via ChatGPT or any other online materials you trust. Bagging is not a model evaluation method but a model training method.

\textbf{(a) Please describe the procedure of Bagging.}

Bagging, short for Bootstrap Aggregating, is an ensemble learning technique designed to improve the stability and accuracy of machine learning algorithms. The procedure of bagging can be described in the following steps:

\begin{itemize}
    \item \textbf{Bootstrap Sampling}: Create multiple subsets of the original training dataset by randomly sampling with replacement. Each subset, known as a bootstrap sample, may contain duplicate instances and will typically be of the same size as the original dataset.

    \item \textbf{Train Base Learners}: For each bootstrap sample, train a separate base learner (e.g., decision tree, neural network). Each model is trained independently, and the randomness introduced by bootstrap sampling ensures that the models capture different patterns in the data.

    \item \textbf{Aggregate Predictions}: Once all base learners are trained, aggregate their predictions to make a final prediction. For classification tasks, the most common aggregation method is majority voting, where the class predicted by the majority of the models is chosen. For regression tasks, the predictions are typically averaged.

    \item \textbf{Final Model Output}: The output of the bagging procedure is a single ensemble model that combines the strengths of all the individual base learners, often leading to improved performance and reduced variance compared to any single model.

    \item \textbf{Evaluation and Tuning}: After constructing the bagging ensemble, evaluate its performance on a validation dataset. Hyperparameters, such as the number of base learners or the type of base learner, can be tuned to optimize performance further.
\end{itemize}

\textbf{(b) Why Bagging generally reduce model variance?}

Bagging (Bootstrap Aggregating) generally reduces model variance due to the following reasons:

\begin{itemize}
    \item \textbf{Independence of Base Learners}: By training multiple models on different bootstrap samples of the data, bagging introduces diversity among the base learners. Each model captures different aspects of the data, which helps to smooth out the predictions when aggregated.

    \item \textbf{Averaging Effect}: In regression tasks, the predictions from the individual models are averaged, while in classification tasks, majority voting is used. This aggregation process tends to cancel out the errors made by individual models. If one model makes a high variance error due to a particular subset of data, other models are less likely to make the same mistake, leading to a more stable overall prediction.

    \item \textbf{Reduction of Overfitting}: Models with high variance often overfit the training data, capturing noise and specific patterns that do not generalize well to new data. Bagging mitigates this effect by combining the outputs of multiple models, each trained on slightly different data, which can lead to a more generalized model that performs better on unseen data.

    \item \textbf{Error Reduction through Diversity}: When base learners are trained on different samples, they are likely to make different errors. The aggregation of these diverse predictions helps to reduce the overall error, resulting in lower variance in the ensemble model compared to individual models.

    \item \textbf{Robustness to Outliers}: Bagging is more robust to outliers since the influence of any single data point is diminished across the ensemble. As each model sees a different subset of the data, the impact of outliers on the final prediction is reduced.
\end{itemize}

\subsection{Support Vector Machine}
\textbf{(a) [10 pts] The following is the primal formulation of L2 SVM (with the squared slack variables), which is a variant of the standard SVM that we discussed in our lecture. }

\begin{align}
    \min_{w, b, \xi} \quad & \frac{1}{2} \mathbf{w}^T \mathbf{w} + \frac{C}{2} \sum_{i=1}^{N} \xi_i^2 \label{eq:svm_objective} \\
    \text{s.t.} \quad & y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1 - \xi_i, \quad i \in \{1, \ldots, N\}, \nonumber \\
    & \xi_i \geq 0, \quad i \in \{1, \ldots, N\} \nonumber
\end{align}

If we remove the last constraints \((\xi_i \geq 0)\), we might get a simpler optimization problem:

\begin{align}
    \min_{w, b, \xi} \quad & \frac{1}{2} \mathbf{w}^T \mathbf{w} + \frac{C}{2} \sum_{i=1}^{N} \xi_i^2 \label{eq:svm_objective2} \\
    \text{s.t.} \quad & y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1 - \xi_i, \quad i \in \{1, \ldots, N\}. \nonumber
\end{align}


\textbf{Please provide the Lagrangian of the above simplified formulation.}

Given the simplified formulation, we can write the Lagrangian as:
\begin{align}
  \mathcal{L}(w, b, \xi, \alpha) &= \frac{1}{2} \mathbf{w}^T \mathbf{w} + \frac{C}{2} \sum_{i=1}^{N} \xi_i^2 - \sum_{i=1}^{N} \alpha_i \left( y_i (\mathbf{w}^T \mathbf{x}_i + b) - (1 - \xi_i) \right) \label{eq:q2_a}
\end{align}

where $\alpha_i \geq 0$ are the Lagrange multipliers.
%- \(\alpha_i \geq 0\) are the Lagrange multipliers corresponding to each constraint.

%The Lagrangian combines the original objective function and the penalty for violating the constraints, which will be optimized along with the primal variables \(w\), \(b\), and \(\xi\).

\textbf{(b) [15 pts] Please find the partial derivative of the Lagrangian in (a) with respect to $w$, $b$, and $\xi_i$.
}

We need to take the derivative of the Lagrangian (Eq. \ref{eq:q2_a}) with respect to each variable: $\mathbf{w}$, $b$, and $\xi_i$:

\begin{align}
  \frac{\partial \mathcal{L}}{\partial \mathbf{w}} &= \frac{\partial}{\partial \mathbf{w}} \Bigg( \frac{1}{2} \mathbf{w}^T \mathbf{w} + \frac{C}{2} \sum_{i=1}^{N} \xi_i^2 - \sum_{i=1}^{N} \alpha_i \left( y_i (\mathbf{w}^T \mathbf{x}_i + b) - (1 - \xi_i) \right) \Bigg) \nonumber \label{eq:q2_b_1} \\
  &= \mathbf{w} - \sum_{i=1}^{N} \alpha_i y_i \mathbf{x}_i
\end{align}

\begin{align}
  \frac{\partial \mathcal{L}}{\partial b} &= \frac{\partial}{\partial b} \Bigg( \frac{1}{2} \mathbf{w}^T \mathbf{w} + \frac{C}{2} \sum_{i=1}^{N} \xi_i^2 - \sum_{i=1}^{N} \alpha_i \left( y_i (\mathbf{w}^T \mathbf{x}_i + b) - (1 - \xi_i) \right) \Bigg) \nonumber \label{eq:q2_b_2} \\
  &= -\sum_{i=1}^{N} \alpha_i y_i
\end{align}

\begin{align}
  \frac{\partial \mathcal{L}}{\partial \xi_i} &= \frac{\partial}{\partial \xi_i} \Bigg( \frac{1}{2} \mathbf{w}^T \mathbf{w} + \frac{C}{2} \sum_{i=1}^{N} \xi_i^2 - \sum_{i=1}^{N} \alpha_i \left( y_i (\mathbf{w}^T \mathbf{x}_i + b) - (1 - \xi_i) \right) \Bigg) \nonumber \label{eq:q2_b_3} \\
  &= C \xi_i - \alpha_i
\end{align}

\subsection{Cluster Analysis}

\textbf{[25 pts] Consider the following set of one-dimensional data points: $\{0.1, 0.25, 0.45, 0.55, 0.8, 0.9\}$. All the points are located in the range between $[0, 1]$.}

\textbf{(a) [15 pts] Suppose we apply K-Means clustering to obtain three clusters, A, B, and C. If the initial centroids are located at $\{0, 0.4, 1\}$, respectively, show the cluster assignments and locations of the updated centroids after the first three iterations by filling out the following table. (Hint: although you can directly calculate all distances, if you draw a real axis and put down the points, it might be easier to see the cluster assignment.)}

\begin{table}[h]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \hline
    \textbf{Iter} & \multicolumn{6}{c|}{\textbf{Cluster Assignment of Data Points}} & \multicolumn{3}{c|}{\textbf{Centroid Location}} \\ \hline
         & \textbf{0.1} & \textbf{0.25} & \textbf{0.45} & \textbf{0.55} & \textbf{0.8} & \textbf{0.9} & \textbf{A} & \textbf{B} & \textbf{C} \\ \hline
    1    & A    &  B    &  B    &  B    &  C   &  C   & 0.1  & 0.116  & 0.15  \\ \hline
    2    &  A   &  A    &  B    &   B   &  C   &  C   & 0.175  & 0.5  &  0.85 \\ \hline
    3    & A   &  A    &  B    &   B   &  C   &  C   & 0.175  & 0.5  &  0.85 \\ \hline
  \end{tabular}
  \caption{Final results of this question.}
  \label{tbl:q3_a}
\end{table}

Table \ref{tbl:q3_a} shows the final results from this excercise. My working is below.

For the first iteration, we start with our initial cluster centroids $C_0 = \{0, 0.4, 1\}$, and calculate the Euclidean distance between each point to each centroid. This is shown in Table \ref{tbl:q3_a_1}.

\begin{table}[h]
  \centering
  \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
           & \textbf{0.1} & \textbf{0.25} & \textbf{0.45} & \textbf{0.55} & \textbf{0.8} & \textbf{0.9} \\ \hline
    $A_0 =0$   & \textcolor{blue}{0.1} & 0.25 & 0.45 &0.55  &0.8  & 0.9 \\ \hline
    $B_0 =0.4$ & 0.3 & \textcolor{blue}{0.15} & \textcolor{blue}{0.05}  & \textcolor{blue}{0.15} & 0.4  & 0.5 \\ \hline
    $C_0 =1$  & 0.9 &0.75  &0.55  &0.45    &\textcolor{blue}{0.2}   & \textcolor{blue}{0.1} \\ \hline
  \end{tabular}
  \caption{Euclidean distances between points and centroid $C_0$ locations. The determined cluster for each point, based on shortest distance, is shown in blue.}
  \label{tbl:q3_a_1}
\end{table}

We then determine the new centroid locations, based on the mean of the points within each cluster (Eq. \ref{eq:q3_a_1}), to obtain the new centroid locations.
\begin{align}
  A_1 &= \frac{0.1}{1} = 0.1 \nonumber \\
  B_1 &= \frac{0.25+0.45+0.55}{3} = 0.416 \nonumber \\
  C_1 &= \frac{0.8+0.9}{2} = 0.85\label{eq:q3_a_1}
\end{align}

For the second iteration, we start with our cluster centroids $C_1 = \{0.1, 0.416, 0.85\}$, and calculate the Euclidean distance between each point to each centroid. This is shown in Table \ref{tbl:q3_a_2}.

\begin{table}[h]
  \centering
  \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
           & \textbf{0.1} & \textbf{0.25} & \textbf{0.45} & \textbf{0.55} & \textbf{0.8} & \textbf{0.9} \\ \hline
    $A_1 =0.1$   & \textcolor{blue}{0} & \textcolor{blue}{0.15} & 0.35 &0.45  &0.7  & 0.8 \\ \hline
    $B_1 =0.416$ & 0.316 & 0.166 & \textcolor{blue}{0.034}  & \textcolor{blue}{0.134} & 0.384  & 0.484 \\ \hline
    $C_1 =0.85$  & 0.75 &0.6  &0.4  &0.3 &\textcolor{blue}{0.05} & \textcolor{blue}{0.05} \\ \hline
  \end{tabular}
  \caption{Euclidean distances between points and centroid $C_1$ locations. The determined cluster for each point, based on shortest distance, is shown in blue.}
  \label{tbl:q3_a_2}
\end{table}

We then determine the new centroid locations, based on the mean of the points within each cluster (Eq. \ref{eq:q3_a_2}), to obtain the new centroid locations.
\begin{align}
  A_2 &= \frac{0.1+0.25}{1} = 0.175 \nonumber \\
  B_2 &= \frac{0.45+0.55}{2} = 0.5 \nonumber \\
  C_2 &= \frac{0.8+0.9}{2} = 0.85\label{eq:q3_a_2}
\end{align}

Finally, for the third iteration, we start with our cluster centroids $C_2 = \{0.175, 0.5, 0.85\}$, and calculate the Euclidean distance between each point to each centroid. This is shown in Table \ref{tbl:q3_a_3}.

\begin{table}[h]
  \centering
  \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
           & \textbf{0.1} & \textbf{0.25} & \textbf{0.45} & \textbf{0.55} & \textbf{0.8} & \textbf{0.9} \\ \hline
    $A_2 =0.175$ & \textcolor{blue}{0.075} & \textcolor{blue}{0.075} & 0.275 &0.375  &0.625  & 0.725 \\ \hline
    $B_2 =0.5$ & 0.4 & 0.25 & \textcolor{blue}{0.05}  & \textcolor{blue}{0.05} & 0.3  & 0.4 \\ \hline
    $C_2 =0.85$  & 0.75 &0.6  &0.4  &0.3 &\textcolor{blue}{0.05} & \textcolor{blue}{0.05} \\ \hline
  \end{tabular}
  \caption{Euclidean distances between points and centroid $C_2$ locations. The determined cluster for each point, based on shortest distance, is shown in blue.}
  \label{tbl:q3_a_3}
\end{table}

We then determine the final centroid locations, based on the mean of the points within each cluster (Eq. \ref{eq:q3_a_3}), to obtain the new centroid locations.
\begin{align}
  A_3 &= \frac{0.1+0.25}{1} = 0.175 \nonumber \\
  B_3 &= \frac{0.45+0.55}{2} = 0.5 \nonumber \\
  C_3 &= \frac{0.8+0.9}{2} = 0.85\label{eq:q3_a_3}
\end{align}

\textbf{(b) [5 pts] Find the sum-of-squared errors (SSE) of the clustering after the third iteration. To calculate the SSE, we need to compute the squared distances of each data point from its respective cluster centroid.}

For each cluster, the SSE after the third iteration is given by:

\begin{align}
  \text{SSE}_A &= (0.1-0.175)^2 + (0.25-0.175)^2 = 0.01125 \nonumber \\
  \text{SSE}_B &= (0.45-0.5)^2 + (0.55-0.5)^2 = 0.005 \nonumber \\
  \text{SSE}_C &= (0.8-0.85)^2 + (0.9-0.85)^2 = 0.005 \label{eq:q3b_1}
\end{align}

Then the total SSE is given by:
\begin{align}
  \text{SSE} &= \text{SSE}_A + \text{SSE}_B + \text{SSE}_C \nonumber \\
  &= 0.01125 + 0.005 + 0.005 \nonumber \\
  &= 0.02125 \label{eq:q3b_2}
\end{align}

\textbf{(c) [5 pts] For the dataset given in part (1), is it possible to obtain empty clusters? Why?}

It is possible to obtain empty clusters, either because of poor random initialization, or because of the selected distance metric. For example, if the initial centroid locations were too far from the first few points, then it is possible the distance would have been too great in the first few iterations to be assigned to cluster A. Similarly, if we used some other distance metric, e.g. cosine distance, then that could have affected how each point is assigned to each data cluster.