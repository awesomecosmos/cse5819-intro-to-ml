\section{Problems}
\subsection{Is any of the following problems an example of a supervised learning problem? (You can choose multiple choices). (10 pts)}

\begin{itemize}
    \item[\circledtext{A}] Given a set of patient data, including various features such as breast images, family history, cell nuclei measurements (for example, radius, texture, perimeter, area, smoothness of the cells), and the biopsy results of benign and malignant, the goal is to classify whether a tumor is benign (non-cancerous) or malignant (cancerous).
    \item[\circledtext{B}] Given images of crop (wheat or rice) leaves and biologistâ€™s labels in terms of whether the crop is healthy or suffering from a specific disease (for example, powdery mildew, leaf rust, or bacterial blight), the goal is to classify whether any new plant image is healthy.
    \item[C] Given a large dataset of customer behaviors, such as purchase history, browsing patterns, demographics, and engagement metrics, the goal is to group customers into distinct segments based on similarities in their behavior and preferences. There are no predefined labels for the customer segments.
    \item[\circledtext{D}] Given historical data of a satellite's position, velocity, and various environmental factors (such as gravitational forces, solar radiation pressure, and atmospheric drag), the goal is to predict the satellite's future position in space at a given time.
\end{itemize}

\textcolor{blue}{Options A, B, and D are supervised learning problems.}

\subsection{Is any of the following problems an example of an unsupervised learning problem? (You can choose multiple choices). (10 pts)}

\begin{itemize}
    \item[\circledtext{A}] Given a large dataset of images of faces, each image is represented as a high-dimensional vector (e.g., each pixel in an image is a feature). The goal is to reduce the dimensionality of the data while preserving the most important information, so that the faces can still be recognized or compared with minimal computational cost.
    \item[\circledtext{B}] In a manufacturing plant, machines produce data through sensors, such as temperature, vibration, and sound levels. The goal is to detect unusual patterns or behaviors (anomalies) that could indicate machine faults or maintenance needs. The challenge is that there are no pre-observed labels of normal or faulty operation for every possible scenario.
    \item[C] An autonomous vehicle must learn how to drive through complex environments, such as urban roads, highways, and intersections, while interacting with other vehicles, pedestrians, traffic signals, and obstacles. The goal is to enable the vehicle to make safe and efficient driving decisions, such as when to accelerate, brake, change lanes, or stop at traffic lights.
    \item[\circledtext{D}] In an online learning platform, some students may frequently revisit course materials and may benefit from more practice problems, while some other students need to be challenged with more advanced content. An algorithm might analyze student data (e.g., study habits, online learning activity, quiz completion times, etc.) and groups students with similar learning patterns.
\end{itemize}

\textcolor{blue}{Options A, B, and D are unsupervised learning problems.}

\subsection{Is any of the following methods capable of solving a supervised learning problem? (You can choose multiple choices). (10 pts)}

\begin{itemize}
    \item[\circledtext{A}] Linear regression
    \item[\circledtext{B}] Random forests
    \item[\circledtext{C}] Support vector machines
    \item[\circledtext{D}] K-nearest neighbors
\end{itemize}

\textcolor{blue}{Options A, B, C, and D are all algorithms for solving supervised learning problems.}

\subsection{For a quadratic form as follows (Eq. \ref{eq_q4}), please compute the gradient of this function $f(x)$ where $x$ is a vector in $R^n$, $A$ is a $n \times n$ matrix, $B$ is a vector in $R^n$, and $C$ is a scalar. (20 pts) (Write your derivation. Do not just copy-paste what GPT provides if you use GPT.)}

\begin{equation}
    f(x) = x^T A x + B^T x + C
    \label{eq_q4}
\end{equation}

To find the gradient of $f(x)$, we have:
\begin{align}
    \nabla_x f(x) &= \frac{\partial}{\partial x} (x^T A x + B^T x + C) \nonumber \\
    &= \frac{\partial}{\partial x} x^T A x + \frac{\partial}{\partial x} B^T + \frac{\partial}{\partial x} C
    \label{eq_q4_1}
\end{align} 

Let's take the derivatives of Eq. \ref{eq_q4_1}. First, from the Matrix Cookbook (posted on HuskyCT) Eq. 81, we can use the following identity:
\begin{equation}
    \frac{\partial}{\partial x} x^T A x = (A + A^T) x  
    \label{eq_q4_2}
\end{equation}

Using normal differentiation rules, we have:
\begin{equation}
    \frac{\partial}{\partial x} B^T x = B  
    \label{eq_q4_3}
\end{equation}

and: 
\begin{equation}
    \frac{\partial}{\partial x} C = 0  
    \label{eq_q4_4}
\end{equation}

Therefore, substituting Eq. \ref{eq_q4_2}, \ref{eq_q4_3} and \ref{eq_q4_4} back into Eq. \ref{eq_q4_1}, we find the gradient of $f(x)$ as:
\begin{align}
    \therefore \nabla_x f(x) &= \frac{\partial}{\partial x} x^T A x + \frac{\partial}{\partial x} B^T + \frac{\partial}{\partial x} C \nonumber \\
    &= (A + A^T) x + B + 0 \nonumber \\
    &= (A + A^T) x + B
    \label{eq_q4_5}
\end{align} 

\subsection{The following function (Eq. \ref{eq_q5}) is a quadratic form where $a_{12}=a_{21}$, can you write out its gradient, and compare with the above gradient formula you derived in Problem 4. (10 pts)}

\begin{equation}
    f(x) = a_{11}x_1^2 + a_{12}x_1 x_2 + a_{21}x_2 x_1 + a_{22} x_2^2 + b_1 x_1 + b_2 x_2 + c
    \label{eq_q5}
\end{equation}

Since we are given $a_{12}=a_{21}$, Eq. \ref{eq_q5} simplifies to:

\begin{equation}
    f(x) = a_{11}x_1^2 + 2 a_{12}x_1 x_2 + a_{22} x_2^2 + b_1 x_1 + b_2 x_2 + c
    \label{eq_q5_1}
\end{equation}

We need to find the gradient with respect to $x$. Since there are multiple $x$, i.e. $x_1$ and $x_2$, we take the derivative of Eq. \ref{eq_q5_1} with respect to each $x_1$ and $x_2$.

\begin{equation}
    \frac{\partial f(x)}{\partial x_1} = 2 a_{11}x_1 + 2 a_{12}x_2 + b_1
    \label{eq_q5_2}
\end{equation}

\begin{equation}
    \frac{\partial f(x)}{\partial x_2} = 2 a_{12}x_1 + 2 a_{22}x_2 + b_2
    \label{eq_q5_3}
\end{equation}

Therefore the gradient of $f(x)$ is:

\begin{equation}
    \nabla_x f(x) = \begin{bmatrix}
        \frac{\partial f(x)}{\partial x_1} \\
        \frac{\partial f(x)}{\partial x_2}
    \end{bmatrix}
    = \begin{bmatrix}
        2 a_{11}x_1 + 2 a_{12}x_2 + b_1 \\
        2 a_{12}x_1 + 2 a_{22}x_2 + b_2
    \end{bmatrix}
    \label{eq_q5_4}
\end{equation}

We can re-write Eq. \ref{eq_q5_4} like:
\begin{align}
    \nabla_x f(x) &= 
    \begin{bmatrix}
        2 a_{11}x_1 + 2 a_{12}x_2 \\
        2 a_{12}x_1 + 2 a_{22}x_2
    \end{bmatrix} +
    \begin{bmatrix}
        b_1 \\
        b_2
    \end{bmatrix} \\
    &= 2 
    \begin{bmatrix}
        a_{11} & a_{12} \nonumber \\
        a_{12} & a_{22}
    \end{bmatrix} \begin{bmatrix}
        x_1 \\ x_2
    \end{bmatrix} +
    \begin{bmatrix}
        b_1 \\
        b_2
    \end{bmatrix} \nonumber \\
    &= 2 A x + B
    \label{eq_q5_5}
\end{align}

If we compare Eq. \ref{eq_q5_5} with Eq. \ref{eq_q4_5} derived in Question 4, then given the condition $a_{12}=a_{21}$, both Eq. \ref{eq_q5_5} and Eq. \ref{eq_q4_5} are equivalent to each other, since $a_{12}=a_{21}$ implies a symmetrical matrix, in which case $A = A^T$. Therefore:

\begin{align}
    \therefore \nabla_x f(x) &= (A + A^T) x + B \nonumber \\
    &= (2 A) x + B \nonumber \\
    &= 2 A x + B
    \label{eq_q5_6}
\end{align}

\subsection{For a sigmoid function $\sigma (s)=\frac{1}{1+e^{-s}}$, where $s$ is a scalar, prove that its derivative satisfies: $\sigma'(s) = \sigma(s) \cdot (1-\sigma (s))$. (20pts)}

We have: 
\begin{align}
    \sigma(s) &= \frac{1}{1+e^{-s}} \nonumber \\
    &= (1+e^{-s})^-1
    \label{eq_q6_1}
\end{align}

Using the chain rule:
\begin{align}
    \frac{d \sigma(s)}{d s} &= -1 \cdot (1+e^{-s})^{-2} \cdot \frac{d}{d s} (1+e^{-s}) \nonumber \\
    &= -(1+e^{-s})^{-2} \cdot \frac{d}{d s} (1+e^{-s})
    \label{eq_q6_2}
\end{align}

From Eq. \ref{eq_q6_2}, we have:
\begin{align}
    \frac{d}{d s} (1+e^{-s}) &= \frac{d}{d s} (1) + \frac{d}{d s} (e^{-s}) \nonumber \\
    &= -e^{-s}
    \label{eq_q6_3}
\end{align}

Substituting Eq. \ref{eq_q6_3} back into Eq. \ref{eq_q6_2}, we have:
\begin{align}
    \frac{d \sigma(s)}{d s} &= -(1+e^{-s})^{-2} \cdot -e^{-s} \nonumber \\
    &= (1+e^{-s})^{-2} \cdot e^{-s} \nonumber \\
    &= \frac{e^{-s}}{(1+e^{-s})^2} \nonumber \\
    &= \frac{e^{-s}}{(1+e^{-s})(1+e^{-s})} \nonumber \\
    &= \frac{1}{(1+e^{-s})} \cdot \frac{e^{-s}}{(1+e^{-s})} \nonumber \\
    &= \sigma(s) \cdot \frac{e^{-s}}{(1+e^{-s})}
    \label{eq_q6_4}
\end{align}

Note that we can rewrite:
\begin{align} \label{eq_q6_5}
    \frac{e^{-s}}{(1+e^{-s})} &= 1 - \frac{1}{(1+e^{-s})} = 1 - \sigma (s)  \\
    &= \frac{1(1+e^{-s}) - 1}{(1+e^{-s})} \nonumber \\
    &= \frac{1+e^{-s} - 1}{(1+e^{-s})} \nonumber \\
    &= \frac{e^{-s}}{(1+e^{-s})} \nonumber
\end{align}

Therefore substituting Eq. \ref{eq_q6_5} back into Eq. \ref{eq_q6_4}, we have:
\begin{align}
    \therefore \frac{d \sigma(s)}{d s} &= \sigma(s) \cdot (1 - \sigma (s))
    \label{eq_q6_6}
\end{align}

Therefore we proved that $\sigma'(s) = \sigma(s) \cdot (1-\sigma (s))$.